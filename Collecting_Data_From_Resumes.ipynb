{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collection from Resumes"
      ],
      "metadata": {
        "id": "0D-6d0Y270sJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting Text From Resume"
      ],
      "metadata": {
        "id": "HEfO1ncq7PIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reads text content from word and returns as plain text with spaces (instead of tabs)\n",
        "\n",
        "import docx2txt\n",
        " \n",
        "def extract_text_from_docx(docx_path):\n",
        "    txt = docx2txt.process(docx_path)\n",
        "    if txt:\n",
        "        return txt.replace('\\t', ' ')\n",
        "    return None"
      ],
      "metadata": {
        "id": "vafdZY_-6_sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Collecting Skills"
      ],
      "metadata": {
        "id": "H3rlWnV-7TkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #Natural Language Toolkit (to tokenize)\n",
        "nltk.download('stopwords')\n",
        " \n",
        "# you may read the database from a csv file or some other database\n",
        "SKILLS_DB = [\n",
        "    'machine learning',\n",
        "    'data science',\n",
        "    'python',\n",
        "    'word',\n",
        "    'excel',\n",
        "    'English',\n",
        "]\n",
        " \n",
        " \n",
        "def extract_skills(input_text):\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
        " \n",
        "    # remove the stop words\n",
        "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
        " \n",
        "    # remove the punctuation\n",
        "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
        " \n",
        "    # generate bigrams and trigrams (such as artificial intelligence)\n",
        "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
        " \n",
        "    # we create a set to keep the results in.\n",
        "    found_skills = set()\n",
        " \n",
        "    # we search for each token in our skills database\n",
        "    for token in filtered_tokens:\n",
        "        if token.lower() in SKILLS_DB:\n",
        "            found_skills.add(token)\n",
        " \n",
        "    # we search for each bigram and trigram in our skills database\n",
        "    for ngram in bigrams_trigrams:\n",
        "        if ngram.lower() in SKILLS_DB:\n",
        "            found_skills.add(ngram)\n",
        " \n",
        "    return found_skills"
      ],
      "metadata": {
        "id": "SrIdhwTU65PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Collecting Eduaction"
      ],
      "metadata": {
        "id": "Om63krni7YgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        " \n",
        " \n",
        "RESERVED_WORDS = [\n",
        "    'school',\n",
        "    'college',\n",
        "    'university',\n",
        "    'academy',\n",
        "    'faculty',\n",
        "    'institute',\n",
        "    'facility',\n",
        "    'polytechnic',\n",
        "]\n",
        "\n",
        "def extract_education(input_text):\n",
        "    organizations = []\n",
        " \n",
        "    # first get all the organization names using nltk\n",
        "    for sent in nltk.sent_tokenize(input_text):\n",
        "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
        "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
        " \n",
        "    # we search for each bigram and trigram for reserved words\n",
        "    # (college, university etc...)\n",
        "    education = set()\n",
        "    for org in organizations:\n",
        "        for word in RESERVED_WORDS:\n",
        "            if org.lower().find(word) &gt;= 0:\n",
        "                education.add(org)\n",
        " \n",
        "    return education\n"
      ],
      "metadata": {
        "id": "QPJrYTOV7GQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Automatically extract relevant info from resumes\n",
        " if __name__ == '__main__':\n",
        "    text = extract_text_from_docx('resume.docx')\n",
        "    skills = extract_skills(text)\n",
        "    education_information = extract_education(text)\n",
        " \n",
        "    print(skills)\n",
        "    print(education_information)"
      ],
      "metadata": {
        "id": "iTsIhine65bd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}